# Databricks 14 Days AI Challenge â€“ Day 01

## ğŸ“Œ Overview
This repository documents my learning journey through the **Databricks 14 Days AI Challenge**.

**Day 01** focused on understanding the Databricks platform fundamentals before diving into deeper data engineering and AI workflows.

The primary goal was **platform orientation and mindset building**, rather than heavy coding.

---

## ğŸ¯ What I Learned
- Why **Databricks** is preferred over **Pandas** for large-scale data processing  
- The core idea behind **Lakehouse Architecture**  
- How Databricks separates **storage and compute**  
- How **Apache Spark** fits into the Databricks ecosystem  
- Where **notebooks, clusters, and data** live inside Databricks  

---

## ğŸ› ï¸ Tasks Completed
- Created a **Databricks Community Edition** account  
- Explored key platform components:
  - Workspace  
  - Compute (clusters)  
  - Data Explorer  
- Created my first Databricks notebook  
- Ran basic **PySpark commands** to validate the environment  

---

## ğŸ§  Key Takeaway
Day 01 was intentionally kept **light on code** and **heavy on understanding the platform**.

Instead of rushing into transformations or analytics, the focus was on:
- Knowing **where** code runs  
- Understanding **why** Databricks exists  
- Building a strong foundation for the upcoming days  

---

## ğŸš€ Next Steps
- Day 02: Working with real datasets using PySpark  
- Gradually moving from exploration to structured data processing  

---

## ğŸ“ Notes
- This repository is part of a **learning challenge**, not a production project  
- Code and structure will evolve as the challenge progresses  


